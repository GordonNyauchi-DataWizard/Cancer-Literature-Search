# ğŸ—ï¸ System Architecture

Detailed technical documentation for the Cancer Medical Literature Semantic Search System.

## Table of Contents

1. [High-Level Architecture](#high-level-architecture)
2. [Component Details](#component-details)
3. [Data Flow](#data-flow)
4. [Algorithms](#algorithms)
5. [Storage](#storage)
6. [Performance](#performance)
7. [Scalability](#scalability)
8. [Security](#security)

## High-Level Architecture

### System Layers

The application follows a layered architecture pattern:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PRESENTATION LAYER                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚    CLI     â”‚  â”‚  Streamlit â”‚  â”‚ Python API â”‚           â”‚
â”‚  â”‚   (cli.py) â”‚  â”‚  (app.py)  â”‚  â”‚  (import)  â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  APPLICATION LAYER                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           CancerSearchApp                            â”‚   â”‚
â”‚  â”‚  â€¢ Orchestration logic                               â”‚   â”‚
â”‚  â”‚  â€¢ Index lifecycle management                        â”‚   â”‚
â”‚  â”‚  â€¢ Error handling & validation                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DOMAIN LAYER                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   SemanticIndex      â”‚    â”‚    LLMEnhancer       â”‚      â”‚
â”‚  â”‚  â€¢ Embedding gen     â”‚    â”‚  â€¢ Summarization     â”‚      â”‚
â”‚  â”‚  â€¢ Similarity search â”‚    â”‚  â€¢ Q&A               â”‚      â”‚
â”‚  â”‚  â€¢ Index I/O         â”‚    â”‚  â€¢ Comparison        â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 DATA ACCESS LAYER                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ PDF Reader   â”‚  â”‚ NumPy Arrays â”‚  â”‚   Pickle     â”‚     â”‚
â”‚  â”‚ (PyPDF2)     â”‚  â”‚ (embeddings) â”‚  â”‚  (metadata)  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  EXTERNAL SERVICES                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Sentence Transformersâ”‚    â”‚   Anthropic API      â”‚      â”‚
â”‚  â”‚ (Hugging Face)       â”‚    â”‚   (Claude)           â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Design Patterns

1. **Facade Pattern**: `CancerSearchApp` provides simple interface to complex subsystems
2. **Strategy Pattern**: Different search modes (search, Q&A, compare) use same interface
3. **Singleton Pattern**: Models loaded once and cached
4. **Factory Pattern**: Result formatters for different output types

## Component Details

### 1. Text Processing Pipeline

#### PDF Extraction (`extract_pdf_chunks`)

**Purpose**: Convert PDF documents into searchable text chunks.

**Process**:
```python
PDF â†’ Pages â†’ Raw Text â†’ Clean Text â†’ Overlapping Chunks
```

**Details**:

1. **Page Extraction**
   ```python
   reader = PdfReader(pdf_path)
   for page in reader.pages:
       raw_text = page.extract_text()
   ```
   - Uses PyPDF2 for text extraction
   - Handles multi-page documents
   - Skips empty pages

2. **Text Cleaning**
   ```python
   def clean_text(text):
       text = text.replace("-\n", "")      # Remove hyphenation
       text = text.replace("\n", " ")      # Normalize whitespace
       text = re.sub(r"\s+", " ", text)    # Collapse spaces
       return text.strip()
   ```
   - Removes formatting artifacts
   - Normalizes whitespace
   - Preserves sentence boundaries

3. **Chunking Strategy**
   ```
   Document: [-------- 900 chars --------]
                           [-------- 900 chars --------]
                                       [-------- 900 chars --------]
                                           â¬†ï¸
                                       120 char overlap
   ```
   
   **Why overlap?**
   - Prevents sentence splitting across chunks
   - Maintains context at boundaries
   - Improves search recall

**Chunk Metadata**:
```python
{
    "chunk_id": "paper.pdf::p5::c42",  # Unique identifier
    "pdf_file": "paper.pdf",            # Source file
    "page": 5,                          # Page number
    "text": "chunk content..."          # Actual text
}
```

### 2. Embedding Generation

#### Model: Sentence-BERT (all-MiniLM-L6-v2)

**Architecture**:
```
Input Text
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tokenizer   â”‚  Converts text to token IDs
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BERT Model  â”‚  Transformer encoder (6 layers)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pooling     â”‚  Mean pooling over token embeddings
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Normalize   â”‚  L2 normalization
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
384-dimensional vector
```

**Why this model?**
- **Fast**: 6 layers (vs 12 in base BERT)
- **Small**: ~90MB download
- **Accurate**: Fine-tuned on 1B+ sentence pairs
- **Normalized**: Ready for cosine similarity

**Batch Processing**:
```python
# Process in batches for efficiency
for i in range(0, len(texts), BATCH_SIZE):
    batch = texts[i:i+BATCH_SIZE]
    embeddings = model.encode(batch, normalize_embeddings=True)
```

**Performance**:
- ~200 sentences/second on CPU
- ~2000 sentences/second on GPU
- Memory: ~2GB for 10,000 embeddings

### 3. Similarity Search

#### Cosine Similarity

**Formula**:
```
similarity(A, B) = (A Â· B) / (||A|| Ã— ||B||)

Since vectors are normalized (||A|| = ||B|| = 1):
similarity(A, B) = A Â· B  (dot product)
```

**Implementation**:
```python
# Query embedding
query_emb = model.encode([query], normalize_embeddings=True)[0]

# Compute similarities with all documents
similarities = np.dot(embeddings_matrix, query_emb)

# Get top K
top_indices = np.argsort(similarities)[::-1][:top_k]
```

**Complexity**:
- Time: O(n Ã— d) where n=num_chunks, d=embedding_dim
- Space: O(n Ã— d) for storing embeddings

**Why cosine similarity?**
- Measures semantic similarity (direction)
- Invariant to document length
- Fast computation (matrix multiplication)
- Range [0, 1] for normalized vectors

#### Search Scoring

Similarity scores interpretation:
- **0.8-1.0**: Highly relevant (near duplicate)
- **0.6-0.8**: Relevant (same topic)
- **0.4-0.6**: Somewhat relevant (related concepts)
- **0.0-0.4**: Not relevant

### 4. LLM Enhancement

#### Architecture

```python
User Query
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Semantic Search  â”‚  Retrieve top-K chunks
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Context Builder  â”‚  Format chunks for LLM
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt Template  â”‚  Add instructions
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Claude API       â”‚  Generate response
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
Enhanced Answer with Citations
```

#### Prompt Engineering

**Summarization**:
```python
prompt = f"""You are analyzing search results from cancer medical literature.

Here are the top search results:
{context}

Please provide a concise summary (3-5 sentences) of the main findings.
Focus on:
1. Key themes or topics
2. Important findings or conclusions
3. Any notable patterns or connections
"""
```

**Question Answering**:
```python
prompt = f"""You are a medical research assistant specializing in cancer.

Question: {query}

Context from research papers:
{context}

Please answer based on the provided context. Include:
1. A clear, direct answer
2. Key supporting evidence
3. Citations (paper name and page)
4. Any important caveats

If context doesn't contain enough info, say so.
"""
```

**Why RAG (Retrieval-Augmented Generation)?**
- Grounds answers in actual research
- Provides citations for verification
- Reduces hallucination
- Works with current data (no training needed)

### 5. Index Persistence

#### Storage Format

**Files**:
```
index/
â”œâ”€â”€ chunks.pkl          # Pickled list of chunk dictionaries
â”œâ”€â”€ embeddings.npy      # NumPy array (float32)
â””â”€â”€ config.json         # Model configuration
```

**Serialization**:
```python
# Chunks (metadata)
with open("chunks.pkl", "wb") as f:
    pickle.dump(chunks, f)

# Embeddings (vectors)
np.save("embeddings.npy", embeddings)

# Config (JSON)
json.dump({"model_name": "..."}, f)
```

**Load Time**:
- Chunks: ~100ms for 10,000 chunks
- Embeddings: ~500ms for 10,000 Ã— 384 float32
- Total: <1 second for typical corpus

## Data Flow

### Indexing Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PDF Files   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ extract_pdf_chunks  â”‚  Extract and chunk text
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunks List         â”‚  [chunk1, chunk2, ...]
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SentenceTransformer â”‚  Generate embeddings
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embeddings Matrix   â”‚  (N Ã— 384) float32
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Save to Disk        â”‚  Persist for reuse
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Search Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Query  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Encode Query        â”‚  query â†’ embedding
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Compute Similarity  â”‚  dot product with all chunks
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Top-K Selection     â”‚  argsort + slice
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Format Results      â”‚  Add metadata + scores
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Enhancement     â”‚  (Optional) Summarize/Answer
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Return to User      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Algorithms

### 1. Overlapping Window Chunking

**Algorithm**:
```python
def chunk_with_overlap(text, max_chars, overlap):
    chunks = []
    start = 0
    
    while start < len(text):
        # Extract chunk
        end = min(start + max_chars, len(text))
        chunk = text[start:end]
        chunks.append(chunk)
        
        # Move start with overlap
        if end == len(text):
            break
        start = end - overlap
    
    return chunks
```

**Time Complexity**: O(n) where n = text length
**Space Complexity**: O(n) for storing chunks

### 2. Batch Embedding Generation

**Algorithm**:
```python
def generate_embeddings(texts, model, batch_size):
    embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        batch_emb = model.encode(batch, normalize_embeddings=True)
        embeddings.append(batch_emb)
    
    return np.vstack(embeddings)
```

**Time Complexity**: O(n Ã— m) where n=num_texts, m=encoding_time
**Space Complexity**: O(n Ã— d) where d=embedding_dim

### 3. Top-K Similarity Search

**Algorithm**:
```python
def search_top_k(query_emb, doc_embeddings, k):
    # Compute all similarities
    similarities = np.dot(doc_embeddings, query_emb)
    
    # Get top K indices
    top_k_idx = np.argsort(similarities)[::-1][:k]
    
    return top_k_idx, similarities[top_k_idx]
```

**Time Complexity**: 
- Dot product: O(n Ã— d)
- Argsort: O(n log n)
- Total: O(n log n)

**Optimization**: For very large corpora, use approximate nearest neighbor (ANN) algorithms like FAISS.

## Storage

### Disk Usage

**Per 100 papers** (typical):
- PDFs: ~500 MB
- Chunks: ~2 MB (pickle)
- Embeddings: ~15 MB (float32)
- **Total**: ~520 MB

**Scaling**:
| Papers | Chunks | Embeddings | Total Size |
|--------|--------|------------|------------|
| 100    | 10K    | 15 MB      | ~520 MB    |
| 1,000  | 100K   | 150 MB     | ~5 GB      |
| 10,000 | 1M     | 1.5 GB     | ~50 GB     |

### Memory Usage

**Runtime**:
- Model: ~400 MB
- Embeddings: ~15 MB per 10K chunks
- Chunks metadata: ~2 MB per 10K chunks
- **Typical**: <500 MB for 10K chunks

## Performance

### Benchmarks

**Hardware**: Intel i7, 16GB RAM, No GPU

| Operation              | Time        | Notes                    |
|------------------------|-------------|--------------------------|
| PDF extraction         | ~0.5s/page  | Depends on PDF quality   |
| Embedding generation   | ~5s/1000    | CPU only                 |
| Index saving           | ~1s         | 10K chunks               |
| Index loading          | ~0.5s       | 10K chunks               |
| Search query           | ~50ms       | 10K chunks               |
| LLM summary            | ~2-5s       | Depends on API latency   |

### Optimization Strategies

1. **Use GPU**: 10Ã— faster embedding generation
   ```python
   model = SentenceTransformer('...', device='cuda')
   ```

2. **Increase batch size**: More efficient GPU utilization
   ```python
   BATCH_SIZE = 128  # if using GPU
   ```

3. **Use FAISS**: For >1M chunks
   ```python
   import faiss
   index = faiss.IndexFlatIP(embedding_dim)
   index.add(embeddings)
   ```

4. **Cache results**: Memoize frequent queries

5. **Async LLM calls**: Don't block on API requests

## Scalability

### Current Limitations

- **Memory**: Loads all embeddings into RAM
- **Search**: O(n) linear search
- **Single machine**: No distributed processing

### Scaling Strategies

#### 1. Horizontal Scaling (>1M documents)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Shard 1    â”‚     â”‚  Shard 2    â”‚     â”‚  Shard 3    â”‚
â”‚  (0-333K)   â”‚     â”‚ (333K-666K) â”‚     â”‚ (666K-1M)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚                   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Aggregator  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2. Use Vector Database

Replace NumPy with specialized DB:
- **Pinecone**: Managed vector DB
- **Weaviate**: Open-source vector DB
- **Milvus**: Scalable vector search
- **FAISS**: Facebook's similarity search

#### 3. Implement Caching

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_search(query, top_k):
    return search(query, top_k)
```

#### 4. Async Processing

```python
import asyncio

async def process_pdfs_async(pdf_files):
    tasks = [process_pdf(pdf) for pdf in pdf_files]
    return await asyncio.gather(*tasks)
```

## Security

### Current Implementation

1. **API Key Protection**
   - Store in environment variables
   - Never commit to version control
   - Use `.env` files locally

2. **Input Validation**
   - Sanitize file paths
   - Validate PDF files before processing
   - Limit query length

3. **Error Handling**
   - Catch and log exceptions
   - Graceful degradation
   - User-friendly error messages

### Production Considerations

1. **Authentication**: Add user authentication
2. **Rate Limiting**: Prevent abuse
3. **Access Control**: Restrict sensitive documents
4. **Audit Logging**: Track usage
5. **Encryption**: Encrypt data at rest
6. **HTTPS**: Secure data in transit

## Future Enhancements

### Technical Improvements

1. **Approximate Nearest Neighbor (ANN)**
   - Use FAISS or Annoy
   - 100Ã— faster search
   - Minimal accuracy loss

2. **GPU Acceleration**
   - Batch embedding generation
   - Parallel PDF processing
   - Faster indexing

3. **Incremental Indexing**
   - Add new papers without rebuilding
   - Update-in-place
   - Background processing

4. **Multi-modal Search**
   - Search images in papers
   - Chemical structure search
   - Formula recognition

### Feature Additions

1. **Advanced Filtering**
   - By date, journal, authors
   - By citation count
   - By study type

2. **Personalization**
   - User preferences
   - Search history
   - Recommended papers

3. **Collaboration**
   - Shared collections
   - Annotations
   - Discussion threads

4. **Export/Integration**
   - Export to Zotero/Mendeley
   - Google Scholar integration
   - PubMed sync

---

**Last Updated**: January 2026  
**Version**: 1.0  
**Maintainer**: [Your Name]
